# LLM-Models-Experiments
Trying out with large language models on different applications

As technology advances, we are constantly discovering new ways to push the boundaries of what we thought was possible. Large language models are just one example of how we are utilizing technology to create more intelligent and sophisticated software.

Language Models (LMs) are a class of probabilistic models explicitly tailored to identify and learn statistical patterns in natural language. The primary function of a language model is to calculate the probability that a word succeeds a given input sentence.

![data-src-image-8517f0ad-a19d-4958-b05b-8b2c83295d23](https://github.com/prashanth-githubuser/LLM-Models-Experiments/assets/120344718/413da36b-90d9-4c5b-b05a-8a89ea841355)

In the context of LMs in particular, larger networks with more parameters have been shown to achieve better performance. Intuitively, the more parameters, the greater their “storage capacity”, even though it should be noted that language models do not store information in a way comparable to the standard way storage memory works in computers (hard drives).

Essentially, a higher number of parameters allows the model to “internalize” a greater variety of statistical patterns (via the numerical relationships of its parameters) within the language data they are exposed to. Larger models, however, also require more computational resources and training data to reach their full potential.

